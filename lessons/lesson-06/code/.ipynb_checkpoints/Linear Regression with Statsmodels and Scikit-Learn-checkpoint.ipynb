{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Statsmodels and Scikit-Learn\n",
    "\n",
    "There are many ways to fit a linear regression and in python I find myself commonly using both\n",
    "[scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html) and [statsmodels](http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/ols.html). This notebook demos some common tasks using these libraries:\n",
    "* Linear regressions in both\n",
    "* Using [dummy variables](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "* Multilinear regression\n",
    "* Quadratic and polynomial regressions\n",
    "* Exponential regressions\n",
    "\n",
    "You can also create [polynomial fits with numpy](http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) and more [general curve fits with scipy](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html).\n",
    "\n",
    "To get started let's load in the libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first few examples we'll use the famous [Iris dataset](http://archive.ics.uci.edu/ml/datasets/Iris). Seaborn provides a few data sets including this one. Let's load the data and take a quick look using [Seaborn's pairplot](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas dataframe\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby('species').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot of the data using seaborn\n",
    "sns.pairplot(iris, hue=\"species\")\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a pretty strong linear relationship between `petal_length` and `petal_width`. Let's fit a linear regression. Seaborn can [plot the data with a regression line](https://stanford.edu/~mwaskom/software/seaborn/tutorial/regression.html) so let's do that first (but it doesn't give us much in the way of statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"petal_length\", y=\"petal_width\", data=iris)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use scikit-learn to find the best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import linear_model\n",
    "\n",
    "X = iris[[\"petal_length\"]]\n",
    "y = iris[\"petal_width\"]\n",
    "\n",
    "# Fit the linear model\n",
    "model = linear_model.LinearRegression()\n",
    "results = model.fit(X, y)\n",
    "\n",
    "# Print the coefficients\n",
    "print results.intercept_, results.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our best fit line is:\n",
    "$$y = a + b x$$\n",
    "where $a = -0.363075521319$ and $b = 0.41575542$.\n",
    "\n",
    "Next let's use `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# Note the swap of X and y\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "# Statsmodels gives R-like statistical output\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely you'll note that this model doesn't include an intercept by default like scikit-learn does. There's an easy way to do this using numpy's [Vandermonde matrix function](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.vander.html) `numpy.vander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 2, 3]\n",
    "np.vander(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `np.vander` gives us the powers of the input matrix or array. We can use it to simply add a constant row for the intercept (zeroth power) or to do polynomial fits (larger exponents). First let's redo the statsmodels fit with an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"petal_length\"]\n",
    "print X\n",
    "X = np.vander(X, 2) # add a constant row for the intercept\n",
    "print X\n",
    "y = iris[\"petal_width\"]\n",
    "\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coefficients are almost identical to what we saw before with scikit-learn, and the fit is pretty good ($R^2=0.927$). Let's see if adding in the species helps. Since that feature is categorical, we need to use dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(iris[\"species\"])\n",
    "# Add to the original dataframe\n",
    "iris = pd.concat([iris, dummies], axis=1)\n",
    "iris.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a multilinear regression with the dummy variables added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[[\"petal_length\", \"setosa\", \"virginica\"]]\n",
    "X = sm.add_constant(X) # another way to add a constant row for an intercept\n",
    "y = iris[\"petal_width\"]\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it looks like we got a slight improvement from including the dummy variables. The dummy variables have a bigger impact on a fit between `petal_length` and `sepal_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[[\"petal_length\"]]\n",
    "X = sm.add_constant(X)\n",
    "y = iris[\"sepal_length\"]\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[[\"petal_length\", \"setosa\", \"versicolor\", \"virginica\"]]\n",
    "y = iris[\"sepal_length\"]\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Fit\n",
    "\n",
    "Next we look at a data set that needs a quadratic fit. Let's do both a linear and quadratic fit and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: 2 + 0.5 * x + 3 * x ** 2 + 5 * stats.norm.rvs(0, 10)\n",
    "df = pd.DataFrame()\n",
    "df[\"x\"] = list(range(0, 30))\n",
    "df[\"y\"] = map(func, df[\"x\"])\n",
    "df.plot.scatter(x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Fit\n",
    "X = df[\"x\"]\n",
    "X = np.vander(X, 2) # add a constant row for the intercept\n",
    "y = df[\"y\"]\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quadratic Fit\n",
    "X2 = df['x']\n",
    "X2 = np.vander(X2, 3) # add a constant and quadratic term\n",
    "y = df[\"y\"]\n",
    "\n",
    "model2 = sm.OLS(y, X2)\n",
    "results2 = model2.fit()\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the quadratic fit is better. We can plot the residuals in both cases to see how far off the models are in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot true values versus the predictions\n",
    "plt.scatter(df['y'], results.predict(), color=\"g\", label=\"Linear\")\n",
    "plt.scatter(df['y'], results2.predict(), color=\"b\", label=\"Quadratic\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's a little hard to tell from the plot (since both fits are good), the blue (quadratic) fit is closer to \"y=x\", indicating a closer agreement with the true values and the model's predictions.\n",
    "\n",
    "Higher order polynomial regressions are as easy as increasing the exponent parameter in `numpy.vander`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential functions\n",
    "\n",
    "We can also transform our data before applying linear regression. This allows us to fit functions such as exponentials of the form $y=A e^{k x}$ using linear regression. Here's some exponentially distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func = lambda x: 2.5 * np.exp(0.5 * x) + stats.norm.rvs(0, 0.3)\n",
    "df = pd.DataFrame()\n",
    "df[\"x\"] = np.arange(-1, 4, 0.1)\n",
    "df[\"y\"] = map(func, df[\"x\"])\n",
    "df.plot.scatter(x='x', y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the log of the `y`-variable we get something more linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"log-y\"] = np.log(df[\"y\"])\n",
    "df.plot.scatter(x='x', y='log-y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use linear regression to determine $k$ and $\\log A$, since taking the $\\log$ of both sides of $y = A e^{k x}$ gives us $\\log y = \\log A + k x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[\"x\"]\n",
    "X = sm.add_constant(X)\n",
    "y = df[\"log-y\"]\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the fit is very good."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
