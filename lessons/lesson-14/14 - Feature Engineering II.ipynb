{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.kdnuggets.com/wp-content/uploads/babbage_analytical_engine.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wit. \n",
    "\n",
    "<footer>~ PCA ( quote )</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/agenda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Construction\n",
    "1. Dimensionality Reduction\n",
    "1. Principal Components Analysis (PCA)\n",
    "1. Singular Value Decomposition\n",
    "1. Kernel Methods In PCA\n",
    "\n",
    "**Labs**\n",
    "1. Dimensionality Reduction In Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance and selection can inform you about the objective utility of features, but those features have to come from somewhere. You need to manually create them. This requires spending a lot of time with actual sample data (not aggregates) and thinking about the underlying form of the problem, structures in the data and how best to expose them to predictive modeling algorithms. With tabular data, it often means a mixture of aggregating or combining features to create new features, and decomposing or splitting features to create new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a categorical attribute, like “Item_Color” that can be `Red`, `Blue` or `Unknown`.\n",
    "\n",
    "`Unknown` may be special, but to a model, it looks like just another colour choice. It might be beneficial to better expose this information. You could create a new binary feature called “Has_Color” and assign it a value of “1” when an item has a color and “0” when the color is unknown. Going a step further, you could create a binary feature for each value that `Item_Color` has. This would be three binary attributes: `Is_Red`, `Is_Blue` and `Is_Unknown`.\n",
    "\n",
    "These additional features could be used instead of the `Item_Color` feature (if you wanted to try a simpler linear model) or in addition to it (if you wanted to get more out of something like a decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "color_list = ['Red','Green','Blue', 'Unknow', None]\n",
    "df = pd.DataFrame(index=range(0,100),columns=['colour'])\n",
    "for x in df.index.values:\n",
    "    df.set_value(x,'colour',random.choice(color_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Decompose Categorical Attributes\n",
    "\n",
    "1. Based on the above `df`, create a binary feature 'has_colour' which indicates whether a row has a valid colour value or not.\n",
    "1. Based on the above `df`, create dummy variables based on the `colours` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "base64.b64encode(\"Try it first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "answer = 'Y29sb3VyX25hbWVzID0gWydSZWQnLCdHcmVlbicsJ0JsdWUnXQpkZlsnaGFzX2NvbG91ciddID0gZGYuY29sb3VyLmlzaW4oY29sb3VyX25hbWVzKS5hc3R5cGUoJ2ludCcpCnBkLmNvbmNhdChbZGYsIHBkLmdldF9kdW1taWVzKGRmLmNvbG91cildLCBheGlzPTEp'\n",
    "answer = 'VHJ5IGl0IGZpcnN0IQ=='\n",
    "for line in base64.b64decode(answer).split('\\n'):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose a Date-Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A date-time contains a lot of information that can be difficult for a model to take advantage of in it’s native form, such as ISO 8601 (i.e. 2014-09-20T20:45:40Z). If you suspect there are relationships between times and other attributes, you can decompose a date-time into constituent parts that may allow models to discover and exploit these relationships.\n",
    "\n",
    "For example, you may suspect that there is a relationship between the time of day and other attributes. You could create a new numerical feature called `Hour_of_Day` for the hour that might help a regression model. You could create a new ordinal feature called `Part_Of_Day` with 4 values `Morning`, `Midday`, `Afternoon`, `Night` with whatever hour boundaries you think are relevant. This might be useful for a decision tree.\n",
    "\n",
    "You can use similar approaches to pick out time of week relationships, time of month relationships and various structures of seasonality across a year. Date-times are rich in structure and if you suspect there is time dependence in your data, take your time and tease them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_date(start, end, seed):\n",
    "    start, end = pd.Timestamp(start), pd.Timestamp(end)\n",
    "    delta = (end - start).total_seconds()\n",
    "    np.random.seed(seed)\n",
    "    offset_sec = np.random.uniform(0., delta)\n",
    "    offset = pd.offsets.Second(offset_sec)\n",
    "    t = start + offset\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With specified timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[random_date(start=\"1/1/2012 1:30 PM\", end=\"1/1/2019 4:50 AM\", seed=seed) for seed in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on pandas datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Past 10 days\n",
    "[random_date(pd.datetime.now() - pd.offsets.Day(10), pd.datetime.now(), seed=seed) for seed in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.random as nprnd\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "dates = [random_date(\"1/1/2015\", pd.datetime.now(), seed) for seed in range(sample_size)]\n",
    "counter = nprnd.randint(50, size=sample_size).tolist()\n",
    "the_type = nprnd.randint(2, size=sample_size).tolist()\n",
    "df_date = pd.DataFrame(data={\"date\":dates, \"counter\":counter, \"the_type\":the_type})\n",
    "df_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Decompose Datetime\n",
    "\n",
    "1. Using `df_date`, parse the date out into `year`, `month` and `day` features\n",
    "1. Using `df_date`, create dummy variables for days of the week.\n",
    "1. Using `df_date`, create a new variables which indicates whether the instance is a weekday or not.\n",
    "1. Using `df_date`, sort all the instances by date, and create a new variable which indicates how much time (in seconds) has elapsed since the last instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deeper look at [drawing features from time series patterns](http://datascience.stackexchange.com/questions/2368/machine-learning-features-engineering-from-date-time-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = 'CnBhcnRzID0gWyd5ZWFyJywnbW9udGgnLCdkYXknXQoKZm9yIHBhcnQgaW4gcGFydHM6CiAgICBkZl9kYXRlW3BhcnRdID0gZ2V0YXR0cihkZl9kYXRlLmRhdGUuZHQsIHBhcnQpCiAgICAKZGZfZGF0ZQo='\n",
    "answer = 'VHJ5IGl0IGZpcnN0IQ=='\n",
    "for line in base64.b64decode(answer).split('\\n'):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = 'CndlZWtkYXlzID0gWydNb24nLCdUdWUnLCdXZWQnLCdUaHUnLCdGcmknLCdTYXQnLCdTdW4nXQpkZl93ZWVrZGF5cyA9IHBkLmdldF9kdW1taWVzKGRmX2RhdGUuZGF0ZS5kdC5kYXlvZndlZWspCmRmX3dlZWtkYXlzLmNvbHVtbnMgPSB3ZWVrZGF5cwpwZC5jb25jYXQoW2RmX2RhdGUsIGRmX2RhdGVfY29tcCwgZGZfd2Vla2RheXNdLCBheGlzPTEpLmhlYWQoKQo='\n",
    "answer = 'VHJ5IGl0IGZpcnN0IQ=='\n",
    "for line in base64.b64decode(answer).split('\\n'):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = 'Cmlzd2Vla2VuZCA9IGRmX2RhdGUuZGF0ZS5kdC5kYXlvZndlZWsgPiA0CmRmX2lzd2Vla2VuZCA9IHBkLkRhdGFGcmFtZShpc3dlZWtlbmQsIGNvbHVtbnM9Wydpc193ZWVrZW5kJ10sIGR0eXBlPW5wLmludCkKcGQuY29uY2F0KFtkZl9kYXRlLCBkZl9kYXRlX2NvbXAsIGRmX3dlZWtkYXlzLCBkZl9pc3dlZWtlbmRdLCBheGlzPTEpLmhlYWQoKQo='\n",
    "answer = 'VHJ5IGl0IGZpcnN0IQ=='\n",
    "for line in base64.b64decode(answer).split('\\n'):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = 'CmRmX2RhdGVfc29ydGVkID0gZGZfZGF0ZS5zb3J0KFsnZGF0ZSddKQp0aW1lX2RpZmZlcmVuY2UgPSBkZl9kYXRlX3NvcnRlZCAtIGRmX2RhdGVfc29ydGVkLnNoaWZ0KCkKdGltZV9kaWZmZXJlbmNlID0gcGQuRGF0YUZyYW1lKHRpbWVfZGlmZmVyZW5jZS5maWxsbmEoMCkuZGF0ZS5kdC5zZWNvbmRzKQp0aW1lX2RpZmZlcmVuY2UuY29sdW1ucyA9IFsndGltZWRlbHRhJ10KcGQuY29uY2F0KFtkZl9kYXRlLCBkZl9kYXRlX2NvbXAsIGRmX3dlZWtkYXlzLCBkZl9pc3dlZWtlbmQsIHRpbWVfZGlmZmVyZW5jZV0sIGF4aXM9MSkuaGVhZCgpCg=='\n",
    "answer = 'VHJ5IGl0IGZpcnN0IQ=='\n",
    "for line in base64.b64decode(answer).split('\\n'):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "\n",
    "df_date.set_index(\"date\", drop=True, inplace=True)\n",
    "df_date.resample('M').sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_date.resample('M')['counter'].mean().iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_date['2016'].resample('M')['counter'].mean().iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_date['2016-01':].resample('D')['counter'].mean().iplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your data is very likely to contain quantities, which can be reframed to better expose relevant structures. This may be a transform into a new unit or the decomposition of a rate into time and amount components. You may have a quantity like a weight, distance or timing. A linear transform may be useful to regression and other scale dependent methods.\n",
    "\n",
    "For example, you may have `Item_Weight` in grams, with a value like `6289`. You could create a new feature with this quantity in `kilograms` as `6.289` or rounded kilograms like 6. If the domain is shipping data, perhaps kilograms is sufficient or more useful (less noisy) a precision for `Item_Weight`. The `Item_Weight` could be split into two features: `Item_Weight_Kilograms` and `Item_Weight_Remainder_Grams`, with example values of `6` and `289` respectively.\n",
    "\n",
    "There may be domain knowledge that items with a weight above `4` incur a higher taxation rate. That magic domain number could be used to create a new binary feature `Item_Above_4kg` with a value of “1” for our example of 6289 grams.\n",
    "\n",
    "You may also have a quantity stored as a rate or an aggregate quantity for an interval. For example, `Num_Customer_Purchases` aggregated over a year. In this case you may want to go back to the data collection step and create new features in addition to this aggregate and try to expose more temporal structure in the purchases, like perhaps seasonality. For example, the following new binary features could be created: `Purchases_Summer`, `Purchases_Fall`, `Purchases_Winter` and `Purchases_Spring`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a straight line 100 yards long and you dropped a penny somewhere on it. It wouldn't be too hard to find. You walk along the line and it takes two minutes.\n",
    "\n",
    "Now let's say you have a square 100 yards on each side and you dropped a penny somewhere on it. It would be pretty hard, like searching across two football fields stuck together. It could take days.\n",
    "\n",
    "Now a cube 100 yards across. That's like searching a 30-story building the size of a football stadium. Ugh.\n",
    "\n",
    "The difficulty of searching through the space gets a *lot* harder as you have more dimensions. You might not realize this intuitively when it's just stated in mathematical formulas, since they all have the same \"width\". That's the curse of dimensionality. It gets to have a name because it is unintuitive, useful, and yet simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/curse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Curse of Dimensionality addresses the difficulty of dealing with multivariate data. It warns us that, for a set of data in high dimensions, **local neighborhoods are almost certainly empty** of data points and neighborhoods that are not empty are almost certainly not local. \n",
    "\n",
    "In explaining this result, biostatistician Jeff Leek thought of clever a demonstration and got his graduate student Prasad Patil to build an interactive program to illustrate the Curse. In the screen shots above, samples of 100 points are randomly and uniformly generated in 1,2,3, and 4 dimensions in the unit cube. Subsets are examined in cubes with edge length of 1/2. In 1-dimension, the simulation contained 55% of the data in a line segment of length 1/2 (expected is, of course, 50%). In 2-dimensions, the simulation contained 31 % in a square with sides of length 1/2 (expected is 25%). In 3-dimensions, the simulation contained 14% in a cube with sides of length 1/2 (expected is 12.5%). And finally, the simulation contained just 4% of the data in a 4-D cube with sides of length 1/2 (expected is 6.25%).\n",
    "\n",
    "As the dimension grows, smaller and smaller percentages of the data can be found in regions with linear dimensions, that our low dimensional intuition tells us, are not small. **Balancing the variance of a large neighborhood with the low bias of a small neighborhood is incredibly difficult** in high dimensions. This is a nice way to help visualize the Curse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High dimensionality makes clustering hard, because having lots of dimensions means that everything is \"far away\" from each other. It's hard to know what true distance means when you have so many dimensions. That's why it's often helpful to perform PCA to reduce dimensionality before clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering methods break down\n",
    "* Our feature space becomes vastly larger than our available sample\n",
    "* Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction is a set of techniques for reducing the size in terms of features records and/or bytes of the dataset under examination. In general the idea is to regard the dataset as a matrix and to decompose the matrix into simpler meaningful pieces. Dimensionality reduction is frequently performed as a pre-­‐processing step before another learning algorithm is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivations for dimensionality  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features in our dataset can be difficult to manage or even misleading e.g if the relationships are actually simpler than they appearFor example suppose we have a dataset with some features that are related to each other Ideally we would like to eliminate this redundancy and consolidate the number of variables we’re looking at To say this more intuitively we want to go from a more complex representation of our data to a less complex one while retaining as much of the signal in our data as possible We can do this by looking at our data “from another angle” In doing this we tease out the principal components of our data. If these relationships are linear then we can use well-established techniques like PCA/SVD.\n",
    "\n",
    "We’d like to analyze the data using the most meaningful basis (or coordinates) possible. More precisely: given an `n x d` matrix `A` (encoding `n` observations of a $d$-­‐dimensional random variable), we want to find a $k$-­‐dimensional representation of $A$ ( $k < d$) that captures the information in the original data, according to some criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * reduce computational expense\n",
    " * reduce susceptibility to overfitting\n",
    " * reduce noise in the dataset\n",
    " * enhance our intuition feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Approaches to Dimensionality Reduction\n",
    "\n",
    " * **selecting a subset of features** using an external criterion ( filter) or the learning algo accuracy itself (wrapper)\n",
    " * **feature extraction** – mapping the features to a lower dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic models (document clustering)\n",
    "* Image recognition/computer vision\n",
    "* Bioinformatics (microarray analysis)\n",
    "* Speech recognition\n",
    "* Astronomy (spectral data analysis)\n",
    "* Recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is a dimension reduction technique that can be used on a matrix of any dimensions. This procedure produces a new basis, each of whose components retain as much variance from the original data as possible. The PCA of a matrix $X$ boils down to the eigenvalue decomposition of the covariance matrix of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitive Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dr1.png)\n",
    "![](assets/dr2.png)\n",
    "![](assets/dr3.png)\n",
    "![](assets/dr4.png)\n",
    "![](assets/dr5.png)\n",
    "![](assets/dr6.png)\n",
    "![](assets/dr7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](assets/featureextraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](assets/PCA_plot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Principle 1: In general high correlation between variables is a telltale sign of high redundancy in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about noise in the data? An assumption of PCA is that we have a reasonably high signal to noise ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Principle 2: The most important dynamics are the ones with the largest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with PCA you will want to ask yourself whether you believe this\n",
    "principle for your data. For many data sets it’s fine, but it’s worth thinking about before you throw away the small variance components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "url = 'http://setosa.io/ev/principal-component-analysis/'\n",
    "IFrame(url, 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `USArrests` dataset. It's a data frame with 50 rows (USA states) and 4 columns containing information about violent crime rates by US State. Since most of the times the variables are measured in different scales, the PCA must be performed with standardized data (mean = 0, variance = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./USArrests.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalise Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm = (df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.precision',4)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "df_norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\", context=\"talk\")\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df_norm)\n",
    "\n",
    "labels = ['pc'+ str(pc) for pc in range(1,5)]\n",
    "var = pca.explained_variance_ratio_ / max(pca.explained_variance_)\n",
    "\n",
    "plt.plot(var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.explained_variance_ratio_).iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.explained_variance_ratio_).cumsum().iplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot method returns a plot of the variances (y-axis) associated with the PCs (x-axis). The Figure above is useful to decide how many PCs to retain for further analysis. In this simple case with only 4 PCs this is not a hard task and we can see that the first two PCs explain most of the variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(pca.transform(df_norm), columns=labels)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import us\n",
    "state_codes = [us.states.lookup(state.decode(\"UTF-8\")).abbr for state in df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random \n",
    "x = df_pca.pc1\n",
    "y = df_pca.pc2\n",
    "labels = list(df.index)\n",
    "\n",
    "# x = df_norm.Murder\n",
    "# y = df_norm.Rape\n",
    "# df_norm.plot(kind = 'scatter', x = 'Murder', y = 'Rape');\n",
    "df_pca.plot(kind = 'scatter', x = 'pc1', y = 'pc2');\n",
    "for label, x, y in zip(state_codes, x, y):\n",
    "    plt.annotate(\n",
    "        label, \n",
    "        xy = (x, y), xytext = (-20, 10),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'pink', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0.5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circle of Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcs = ['pc1','pc2']\n",
    "corrs = [[np.corrcoef(df_pca[pc], df[feat])[0][1] * -1 for feat in df.columns] for pc in pcs]    \n",
    "dfs = pd.DataFrame(corrs, columns=df.columns)\n",
    "dfs.index = pcs\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcs = ['pc1','pc2']\n",
    "corrs = [[np.corrcoef(df_pca[pc], df[feat])[0][1] * -1 for feat in df.columns] for pc in pcs]    \n",
    "dfs = pd.DataFrame(corrs, columns=df.columns)\n",
    "dfs.index = pcs\n",
    "dfs.T.plot(kind='scatter',x='pc1',y='pc2')\n",
    "for label, x, y in zip(df.columns, dfs.T.pc1, dfs.T.pc2):\n",
    "    plt.annotate(\n",
    "        label, \n",
    "        xy = (x, y), xytext = (-20, 10),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'pink', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0.5'))\n",
    "    plt.arrow( 0, 0, x, y, fc=\"k\", ec=\"k\")\n",
    "    \n",
    "circle1=plt.Circle((0,0),1,color='#3490e9', alpha=0.3)\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(circle1)\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((-1.1,1.1,-1.1,1.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcs = ['pc3','pc4']\n",
    "corrs = [[np.corrcoef(df_pca[pc], df[feat])[0][1] * -1 for feat in df.columns] for pc in pcs]    \n",
    "dfs = pd.DataFrame(corrs, columns=df.columns)\n",
    "dfs.index = pcs\n",
    "dfs.T.plot(kind='scatter',x='pc3',y='pc4')\n",
    "for label, x, y in zip(df.columns, dfs.T.pc3, dfs.T.pc4):\n",
    "    plt.annotate(\n",
    "        label, \n",
    "        xy = (x, y), xytext = (-20, 10),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'pink', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0.5'))\n",
    "    plt.arrow( 0, 0, x, y, fc=\"k\", ec=\"k\")\n",
    "    \n",
    "circle1=plt.Circle((0,0),1,color='#3490e9', alpha=0.3)\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(circle1)\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((-1.1,1.1,-1.1,1.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)** tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "for c, i, target_name in zip(\"rgb\", [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title = 'PCA of IRIS dataset'\n",
    "\n",
    "plt.figure()\n",
    "for c, i, target_name in zip(\"rgb\", [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title = 'LDA of IRIS dataset'\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "sk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Methods in PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three standard kernels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* linear: $$K(x,x’) = x^{T}x$$\n",
    "* polynomial: $$K(x,x’) = (x^{T}x’ + 1)^{d}$$\n",
    "* gaussian (rbf): $$K(x,x’) = exp(- \\gamma ||x - x’||^{2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can use kernels methods to produce new clarity around the structure of the data. In particular, KPCA is most often used for image de-noising and pattern recognition (or commonly novelty detection). KPCA is particularly useful for extracting nonlinear features, though like standard PCA, the interpretation is not always straightforward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/MLSS-2012-Fukumizu-Kernel-Methods-for-Statistical-Learning_034.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(2, 2, 1, aspect='equal')\n",
    "plt.title = \"Original space\"\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.plot(X[reds, 0], X[reds, 1], \"ro\")\n",
    "plt.plot(X[blues, 0], X[blues, 1], \"bo\")\n",
    "plt.xlabel = \"$x_1$\"\n",
    "plt.ylabel = \"$x_2$\"\n",
    "\n",
    "X1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n",
    "# projection on the first principal component (in the phi space)\n",
    "Z_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)\n",
    "plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n",
    "\n",
    "plt.subplot(2, 2, 2, aspect='equal')\n",
    "plt.plot(X_pca[reds, 0], X_pca[reds, 1], \"ro\")\n",
    "plt.plot(X_pca[blues, 0], X_pca[blues, 1], \"bo\")\n",
    "plt.title = \"Projection by PCA\"\n",
    "plt.xlabel = \"1st principal component\"\n",
    "plt.ylabel = \"2nd component\"\n",
    "\n",
    "plt.subplot(2, 2, 3, aspect='equal')\n",
    "plt.plot(X_kpca[reds, 0], X_kpca[reds, 1], \"ro\")\n",
    "plt.plot(X_kpca[blues, 0], X_kpca[blues, 1], \"bo\")\n",
    "plt.title = \"Projection by KPCA\"\n",
    "plt.xlabel = \"1st principal component in space induced by $\\phi$\"\n",
    "plt.ylabel = \"2nd component\"\n",
    "\n",
    "plt.subplot(2, 2, 4, aspect='equal')\n",
    "plt.plot(X_back[reds, 0], X_back[reds, 1], \"ro\")\n",
    "plt.plot(X_back[blues, 0], X_back[blues, 1], \"bo\")\n",
    "plt.title = \"Original space after inverse transform\"\n",
    "plt.xlabel = \"$x_1$\"\n",
    "plt.ylabel = \"$x_2$\"\n",
    "\n",
    "plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Work through the iris data set include in sklearn and determine how many principal components are required to accurately predict the data. Note: you should be able to graph it, so it has to be one or two :)\n",
    "\n",
    "2. Try using a kernel to make the PCs have an even clearer distinction between each class label in the iris data set.\n",
    "\n",
    "3. The data from the KPCA section of lecture is from [this data](https://gist.github.com/tijptjik/9408623). See what you can reproduce using KPCA to accurately predict the \"Wine\" column.\n",
    "\n",
    "4. How can PCA/KPCA be used in your final project? Continue practicing your application of these data sets and see how you can improve your current model implementation.\n",
    "\n",
    "### Optional \n",
    "\n",
    "Using a student login [dataset](data : https://raw.githubusercontent.com/ga-students/DAT_SF_10/master/Labs/Lab6/student_logins.csv), Build a program that can predict the duration time of an online student.\n",
    "\n",
    "#### Details about Data Set:\n",
    "* Data is in student_logins.csv\n",
    "* Each session has start and end time\n",
    "* Students have multiple sessions and sometimes take different\n",
    "classes\n",
    "* Account created date is an indicator of how long that student\n",
    "has been participating in the online class\n",
    "\n",
    "#### Guidelines:\n",
    "* Duration time will need to be calculated using the start and end\n",
    "times\n",
    "* Explain how and why you selected features for you model\n",
    "* Create a measurement of how well the model performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses and misuses of PCA\n",
    "\n",
    "#### Good\n",
    "* To compress the data so it takes up less computer memory / disk space\n",
    "* To reduce the dimensions of the input so as to speed up a learning algorythm (e.g. image processing)\n",
    "* To visualise high-dimensional data (by choosing k = 2 or k = 3)\n",
    "\n",
    "#### Not Recommended\n",
    "* Before trying a learning system without it with raw values\n",
    "* To preventing overfitting (regularisation much better, uses y labels, doesn't throw away variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/resources.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [A Tutorial on Principal Component Analysis](http://shlens.wordpress.com/tutorials/)\n",
    "* [Dimensionality Reduction A Short Tutorial](http://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf)\n",
    "* [Stanford PCA Tutorial](http://ufldl.stanford.edu/wiki/index.php/PCA)\n",
    "* [Aaron's PCA/3d/clustering post](http://planspace.org/2013/02/03/pca-3d-visualization-and-clustering-in-r/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Sample size vs. dimensionality](http://www.cbcb.umd.edu/~salzberg/docs/murthy_thesis/survey/node16.html)\n",
    "* [The Curse of Dimensionality in classification](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "* [Scaling your Data](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Academic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Dimensionality Reduction Methods](http://www.stat-d.si/mz/mz2.1/dambra.pdf)\n",
    "* [A survey of dimension reduction techniques](http://www.cc.gatech.edu/~isbell/tutorials/dimred-survey.pdf)\n",
    "* [Random projection in dimensionality reduction: Applications to image and text data](http://users.ics.aalto.fi/ella/publications/randproj_kdd.pdf)\n",
    "* [t-Distributed Stochastic Neighbor Embedding](http://lvdmaaten.github.io/tsne/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Sample size determination](http://en.wikipedia.org/wiki/Sample_size_determination)\n",
    "* [Optimal number of features as a function of sample size for various classiﬁcation rules](http://bioinformatics.oxfordjournals.org/content/21/8/1509.full.pdf+html)\n",
    "* [Sample size and statistical power considerations in high-dimensionality data settings: a comparative study of classification algorithms](http://link.springer.com/content/pdf/10.1186%2F1471-2105-11-447.pdf)\n",
    "* [Some considerations of classification for high dimension low-sample size data](http://smm.sagepub.com/content/early/2011/11/22/0962280211428387.abstract)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
