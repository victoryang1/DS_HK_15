{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://blog.kaggle.com/wp-content/uploads/2014/07/wind.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Feature Engineering is how you win\n",
    "\n",
    "~ RJ de Texas (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. Machine learning algorithms learn a solution to a problem from sample data. In this context, feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/features-in-ML.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Better features means **better results** - Since the success of all Machine Learning algorithms depends on how you present the data, better features will give you better results.\n",
    "* Better features means **flexibility** - You can choose “the wrong models” (less than optimal) and still get good results. Most models can pick up on good structure in data. The flexibility of good features will allow you to use less complex models that are faster to run, easier to understand and easier to maintain. This is very desirable.\n",
    "* Better features means **simpler models** - With well engineered features, you can choose “the wrong parameters” (less than optimal) and still get good results, for much the same reasons. You do not need to work as hard to pick the right models and the most optimized parameters. With good features, you are closer to the underlying problem and a representation of all the data you have available and could use to best characterize that underlying problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular data is described in terms of observations or instances (rows) that are made up of variables or attributes (columns). An attribute _could_ be a feature. The idea of a feature, separate from an attribute, makes more sense in the context of a problem. **A feature is an attribute that is useful or meaningful to your problem**. It is an important part of an observation for learning about the structure of the problem that is being modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/62020_wl_compvision_fig1_wl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer vision, an image is an observation, but a feature could be a line in the image. In natural language processing, a document or a tweet could be an observation, and a phrase or word count could be a feature. In speech recognition, an utterance could be an observation, but a feature might be a single word or phoneme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scoring \n",
    "\n",
    "You can objectively estimate the usefulness of features. This can be helpful as a pre-cursor to selecting features. Features are allocated scores and can then be ranked by their scores. Those features with the highest scores can be selected for inclusion in the training dataset, whereas those remaining can be ignored. **A feature may be important if it is highly correlated with the dependent variable** (the thing being predicted). Correlation coefficients and other univariate (each attribute is considered independently) methods are common methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to do Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A picture relevant to our discussion on feature engineering is the stage at which it happens:\n",
    "\n",
    "1. (tasks before here…)\n",
    "1. **Select Data**: Integrate data, de-normalize it into a dataset, collect it together.\n",
    "1. **Preprocess Data**: Format it, clean it, sample it so you can work with it.\n",
    "1. **Transform Data**: Feature Engineer happens here.\n",
    "1. **Model Data**: Create models, evaluate them and tune them.\n",
    "1. (tasks after here…)\n",
    "\n",
    "We may have to go back through these steps as we identify new perspectives on the data. For example, we may have an attribute that is an aggregate field, like a sum. Rather than a single sum, we may decide to create features to describe the quantity by time interval, such as season. We need to step backward in the process through `Preprocessing` and even `Selecting data` to get access to the “real raw data” and create this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to approach Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Brainstorm features** : Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal.\n",
    "1. **Devise features**: Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two.\n",
    "1. **Select features**: Use different feature importance scorings and feature selection methods to prepare one or more “views” for your models to operate upon.\n",
    "1. **Evaluate models**: Estimate model accuracy on unseen data using the chosen features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those attributes that are irrelevant to the problem need to be removed. There will be some features that will be more important than others to the model accuracy. There will also be features that will be redundant in the context of other features. Feature selection addresses these problems by **automatically selecting a subset that are most useful to the problem**. Feature selection algorithms may use a scoring method to rank and choose features, such as correlation or other feature importance methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Redundant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can contain attributes that are highly correlated with each other. Many methods perform better if highly correlated attributes are removed. We'll write a function, `findCorrelation` which will analyze a correlation matrix of your data’s attributes report on attributes that can be removed.\n",
    "\n",
    "The following example loads the Pima Indians Diabetes dataset that contains a number of biological attributes from medical reports. A correlation matrix is created from these attributes and highly correlated attributes are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# URL for the Pima Indians Diabetes dataset (UCI Machine Learning Repository)\n",
    "url = \"http://goo.gl/j0Rvxq\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# download the file\n",
    "df = pd.read_csv(url, names=names)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Information:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "df.corr().iplot(kind='heatmap', colorscale='spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(corr, mask=mask, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cor = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cor.loc[:,:] = np.tril(cor.values, k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cor.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findCorrelation(df, threshold=0.9):\n",
    "    cor = df.corr()\n",
    "    cor.loc[:,:] = np.tril(cor.values, k=-1)\n",
    "    cor = cor.stack()\n",
    "    return cor[abs(cor) > threshold].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findCorrelation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findCorrelation(df, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this case the preg attribute is remove as it correlates highly with the age attribute.\n",
    "\n",
    "Generally, you want to remove attributes with an absolute correlation of 0.9 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Features By Importance\n",
    "\n",
    "The importance of features can be estimated from data by building a model. Some methods like decision trees have a built in mechanism to report on variable importance. For other algorithms, the importance can be estimated using a ROC curve analysis conducted for each attribute.\n",
    "\n",
    "The example below loads the Pima Indians Diabetes dataset and constructs an ExtraTrees (ET) model. The `feature_importances_` is then used to estimate the variable importance, which is printed and plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.ix[:, [1, 7, 5, 6, 0, 2, 3, 4]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "# Build a classification task\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=2500,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "x_labels = df.iloc[:, indices].columns\n",
    "\n",
    "# Print the feature ranking\n",
    "print \"Feature ranking:\"\n",
    "\n",
    "for f in xrange(8):\n",
    "    print \"%d. feature %s (%f)\" % (f + 1, x_labels[f], importances[indices[f]])\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(xrange(8), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "\n",
    "\n",
    "plt.xticks(xrange(8), x_labels)\n",
    "plt.xlim([-1, 8])\n",
    "plt.xticks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that the `plas`, `age` and `mass` attributes are the top 3 most important attributes in the dataset and the `test` attribute is the least important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Feature Selection\n",
    "\n",
    "Automatic feature selection methods can be used to build many models with different subsets of a dataset and identify those attributes that are and are not required to build an accurate model. A popular automatic method for feature selection provided by SciKit Learn which is called **Recursive Feature Elimination Cross Validation** (RFECV).\n",
    "\n",
    "The example below provides an example of the RFE method on the Pima Indians Diabetes dataset. A Random Forest algorithm is used on each iteration to evaluate the model. The algorithm is configured to explore all possible subsets of the attributes. All 8 attributes are selected in this example, although in the plot showing the accuracy of the different attribute subset sizes, we can see that just 4 attributes gives almost comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFECV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, min_samples_leaf=5, n_jobs=-1)\n",
    "\n",
    "rfecv = RFECV(estimator=rf, step=1, cv=3, scoring='roc_auc', verbose=2)\n",
    "\n",
    "selector = rfecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "class RandomForestClassifierWithCoef(RandomForestClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        super(RandomForestClassifierWithCoef, self).fit(*args, **kwargs)\n",
    "        self.coef_ = self.feature_importances_\n",
    "\n",
    "rf = RandomForestClassifierWithCoef(n_estimators=50,\n",
    "                                        min_samples_leaf=5, n_jobs=-1)\n",
    "\n",
    "rfecv = RFECV(estimator=rf, step=1, cv=3, scoring='roc_auc', verbose=2)\n",
    "\n",
    "selector = rfecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(selector.grid_scores_)\n",
    "df_temp.index += 1 \n",
    "\n",
    "df_temp.iplot(title=\"Grid Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've selected all features. Now let's try this on a synthetic dataset to show you what happens when features are dropped. The regression problem Friedman 1. Inputs are 10 independent variables uniformly distributed on the interval [0,1], only 5 out of these 10 are actually used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 10 sin(π x_1 x_2) + 20 (x_3 - 0.5)^2 + 10 x_4 + 5 x_5 + e$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "\n",
    "selector = RFECV(estimator, step=1, cv=5)\n",
    "selector.fit(X, y)\n",
    "\n",
    "print selector.n_features_\n",
    "print selector.support_ \n",
    "print selector.ranking_\n",
    "print selector.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(selector.grid_scores_)\n",
    "df_temp.index += 1 \n",
    "\n",
    "df_temp.iplot(title=\"Grid Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = SVR(kernel=\"linear\")\n",
    "est.fit(X,y)\n",
    "est.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "friedman = pd.DataFrame(X)\n",
    "friedman['y'] = y \n",
    "\n",
    "corr = friedman.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(corr.abs(), mask=mask, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations are far too voluminous in their raw state to be modeled by predictive modeling algorithms directly. Feature extraction is a process of automatically reducing the dimensionality of these types of observations into a much smaller set that can be modelled. For tabular data, this might include projection methods like **Principal Component Analysis** and unsupervised **Clustering** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering for Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Market Segmentation\n",
    "* [Social Network Analysis](https://immersion.media.mit.edu/viz)\n",
    "* Organising Computing Clusters\n",
    "* Astronomical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering for Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Summarization\n",
    "* Compression\n",
    "* Efficiently Finding Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of a cluster as a “potential class”; then the solution to a clustering problem is to programatically determine these classes. The real purpose of clustering is data exploration, so a solution is anything that contributes to your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $K$-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A greedy learner that partitions a data set into $k$ clusters.\n",
    "\n",
    "* *greedy* – captures local structure (depends on initial conditions)\n",
    "* *partition* – performs complete clustering (each point belongs to\n",
    "exactly one cluster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point is assigned to the cluster with the nearest centroid.\n",
    "\n",
    "*centroid* – the mean of the data points in a cluster\n",
    "  * requires continuous (vector-like) features, though see [TopSig](http://eprints.qut.edu.au/43451/) for a binary implementation\n",
    "  * highlights iterative nature of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[Try Me](http://bl.ocks.org/mbostock/raw/4060366/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These partitions are sometimes called _Voronoi cells_, and these maps _Voronoi diagrams_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/plot_kmeans_digits_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important point to keep in mind is that partitions are not scale-invariant! This means that the same data can yield very different clustering results depending on the scale and the units used. Therefore it’s important to think about your data representation before applying a clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs show two different representations of the same data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/kmeans_scale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the $K$-means algorithm works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/kmeans.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selects $K$ centroids ($K$ rows chosen at random)\n",
    "1. Assigns each data point to its closest centroid\n",
    "1. Recalculates the centroids as the average of all data points in a cluster (i.e., the centroids are p-length mean vectors, where p is the number of variables)\n",
    "1. Assigns data points to their closest centroids\n",
    "1. Continues steps 3 and 4 until the observations are not reassigned or the maximum number of iterations (SciKit uses 300 as a default) is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation details for this approach can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $K$-Means in SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since K-means cluster analysis starts with `k` randomly chosen centroids, a different solution can be obtained each time the function is invoked. Use the `random.seed(n)` function to guarantee that the results are reproducible. Additionally, this clustering approach can be sensitive to the initial selection of centroids. The `KMeans()` class has an `n_init` parameter that attempts multiple initial configurations and reports on the best one. For example, adding `n_init=25` will generate `25` initial configurations. This approach is often recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means is algorithmically pretty efficient (time & space complexity is linear in number of records).\n",
    "* It has a hard time dealing with non-[convex clusters](http://pafnuty.wordpress.com/2013/08/14/non-convex-sets-with-k-means-and-hierarchical-clustering/), or data with widely varying shapes and densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/intclusoutnonfastclus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering binary data with K-Means [(should be avoided)](http://www-01.ibm.com/support/docview.wss?uid=swg21477401). K-means will produce means that are no longer binary. Frequent itemset mining may be a better choice in that case.\n",
    "* Difficulties can sometimes be overcome by increasing the value of k and combining subclusters in a post-processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 – CHOOSING INITIAL CENTROIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you choose the initial centroid positions?\n",
    "* Randomly (but may yield divergent behavior)\n",
    "![](assets/K-means_convergence_to_a_local_minimum.png)\n",
    "* Perform alternative clustering task, use resulting centroids as initial k-means centroids\n",
    "* Start with global centroid, choose point at max distance, repeat (but might select outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 – SIMILARITY MEASURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you determine which centroid is the nearest?\n",
    "* The “nearness” criterion is determined by the similarity/distance measure we discussed earlier. This measure makes quantitative inference possible. (Technically, by defining a similarity measure we are mapping our observations into a metric space.)\n",
    "* A similarity measure must satisfy certain general conditions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d(x,y) \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d(x,y) = 0 \\longleftrightarrow x = y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d(x,y) = d(y,x) \\\\ \\text{(symmetry)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d(x,y) + d(y,z) \\geq d(x,z) \\\\\\text{(triangle inequality)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a number of different [similarity measures](http://ag.arizona.edu/classes/rnr555/lecnotes/10.html) to choose from, and in general the right choice depends on the problem.\n",
    "  * For data that takes values in $R^n$ , the typical choice is the [Euclidean distance](http://en.wikipedia.org/wiki/Euclidean_distance):\n",
    "  \n",
    "  $$ d(p,q) = d(q,p) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \\dots + (p_n -q_n)^2} = \\sqrt{\\sum\\limits_{i=1}^n(q_{i}-p_{i})^2 }$$\n",
    "  \n",
    "  * The use of the Manhattan distance is advised in those situations where for example a difference of 1 in the first variable,and of 3 in the second variable is the same as a difference of 2 in the first variable and of 2 in the second:\n",
    "  \n",
    "     $$\\sum\\limits_{i=1}^n(|q_{i}-p_{i}|)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can express different semantics about our data through the choice of metric. Ex: One popular metric for text mining problems (or any problem with sparse binary data) is the [Jaccard Coefficient](http://matpalm.com/resemblance/jaccard_coeff/)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(A,B) = \\frac{A\\cap B}{A\\cup B}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the jaccard index is a simple measure of how similiar two sets are.\n",
    "it's simply the ratio of the size of the intersection of the sets and the size of the union of the sets.\n",
    "\n",
    "eg. \n",
    "\n",
    "    if J(A,B) is jaccard index between sets A and B\n",
    "    and A = {1,2,3}, B = {2,3,4}, C = {4,5,6},\n",
    "    then J(A,B) = 2/4 = 0.5,\n",
    "    and J(A,C) = 0/6 = 0,\n",
    "    and J(B,C) = 1/5 = 0.2\n",
    "\n",
    "so the most \"similiar\" sets are A and B and the least similiar are A and C\n",
    "(note also J(A,A) = J(B,B) = J(C,C) = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applying this metric to a problem expresses the sparse nature of the data, and makes a variety of text mining techniques accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix whose entries $D_{ij}$ contain the values $d(x, y)$ for all $x$ and $y$ is called the distance matrix. The distance matrix contains all of the information we know about the dataset. For this reason, it’s really the choice of metric that determines the definition of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 – OBJECTIVE FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to recompute the positions of the centroids at each iteration of the algorithm. We do so by optimizing an objective function that tells us how “good” the clustering is. The iterative part of the algorithm (recomputing centroids and reassigning points to clusters) explicitly tries to minimize this objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Using the Euclidean distance measure, one typical objective function is the sum of squared errors from each point $x$ to its centroid $c_i$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ SSE = \\sum\\limits_{i=1}^k \\sum\\limits_{x\\in C_{i}} d(x,c_{i})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two clusterings, we will prefer the one with the lower SSE since this means the centroids have converged to better locations (a better local optimum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4 – CONVERGENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate until some stopping criteria are met; in general, suitable convergence is achieved in a small number of steps. Stopping criteria can be based on the centroids (eg, if positions change by no more than $e$) or on the points (eg, if no more than $x$% change clusters between iterations). Recall that, in general, different runs of the algorithm will converge to different local optima (centroid configurations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, k-means will converge to a solution and return a partition of $k$ clusters, even if no natural clusters exist in the data. We will look at two validation metrics useful for partitional clustering, *cohesion* and *separation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohesion and separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](assets/separationcohesion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cohesion* measures clustering effectiveness within a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{C}(C_{i}) = \\sum\\limits_{x \\in C_i} d(x,c_{i}) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Separation* measures clustering effectiveness between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{S}(C_{i},C_{j}) = d(c_{i},c_{j}) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn these values into overall measures of clustering validity by taking a weighted sum over clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{V}_{total} = \\sum\\limits_{1}^K w_{i}\\hat{V}(C_{i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $V$ can be cohesion, separation, or some function of both. The weights can all be set to $1$ (best for k-means), or proportional to the cluster masses (the number of points they contain).\n",
    "\n",
    "Cluster validation measures can be used to identify clusters that should be split or merged, or to identify individual points with disproportionate effect on the overall clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful measure than combines the ideas of cohesion and separation is the [silhouette coefficient](http://pafnuty.wordpress.com/2013/02/04/interpretation-of-silhouette-plots-clustering/). For an individual point $x_i$ , this is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ SC_{i} = \\frac{b_{i}-a_{i}}{max(a_{i},b_{i})} \\\\\n",
    "\\begin{align*}\n",
    "\\text{such that}&: \\\\\n",
    "& a_{i} = \\text{average distance to the points of its own cluster from } x_{i} \\\\\n",
    "& b_{i} = \\text{average distance to the points of the nearest-cluster from } x_{i}\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette coefficient can take values between -1 and 1. In general, we want separation to be high and cohesion to be low. This corresponds to a value of SC close to +1. A negative silhouette coefficient means the cluster radius is larger than the space between clusters, and thus clusters overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/silhouettegraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette coefficient for the cluster $C_i$ is given by the average silhouette coefficient across all points in $C_i$, where $m_{i}$ is the number of points in a cluster :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ SC(C_{i}) = \\frac{1}{m_{i}} \\sum \\limits_{x \\in C_{i}} SC_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall silhouette coefficient is given by the average silhouette coefficient across all points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ SC_{total} = \\frac{1}{k} \\sum \\limits_{1}^{k} SC(C_{i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a summary measure of the overall clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative validation scheme is given by comparing the similarity matrix with an idealized $(0/1)$ similarity matrix that represents the same clustering configuration. This can be done either graphically or using correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/similaritymatrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful application of cluster validation is to determine the best number of clusters for your dataset. We can do this by computing the overall SSE or [intepreting the Silhouette Coefficient](http://pafnuty.wordpress.com/2013/02/04/interpretation-of-silhouette-plots-clustering/) for different values of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/cluster_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine your level of confidence in these validation metrics, we use rely on our statistical methods, eg, by computing frequency distributions for these metrics (over several runs of the algorithm) and determining statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, cluster validation and clustering in general are suggestive techniques that rely on human interpretation to be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* See a very basic example of K-means applied to a clear data set\n",
    "* Work through an implementation of predicting the classes in the iris data set\n",
    "* Application to data we're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Don't show deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# Set some Pandas options\n",
    "pd.set_option('max_columns', 30)\n",
    "pd.set_option('max_rows', 20)\n",
    "\n",
    "# Set some Matplotlib options\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "# Store data in a consistent place\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means with good data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's look at a readable and good application of kmeans with made up data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.random(20) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load modules\n",
    "from sklearn import cluster\n",
    "from numpy import random\n",
    "from pandas import DataFrame, concat\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "classone = DataFrame({\n",
    "    'x' : random.random(20) + 1,\n",
    "    'y' : random.random(20) + 1,\n",
    "    'label' : ['r' for i in range(20)]\n",
    "})\n",
    "classtwo = DataFrame({\n",
    "    'x' :random.random(20) + 1,\n",
    "    'y' : random.random(20) + 3,\n",
    "    'label' : ['g' for i in range(20)]\n",
    "})\n",
    "classthree = DataFrame({\n",
    "    'x' :random.random(20) + 3,\n",
    "    'y' : random.random(20) + 1,\n",
    "    'label' : ['b' for i in range(20)]\n",
    "})\n",
    "classfour = DataFrame({\n",
    "    'x' :random.random(20) + 3,\n",
    "    'y' : random.random(20) + 3,\n",
    "    'label' : ['purple' for i in range(20)]\n",
    "})\n",
    "\n",
    "data = concat([classone, classtwo, classthree, classfour])\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRanDF(x_offset, y_offset, label, the_number = 20):\n",
    "    x = random.random(the_number) + x_offset\n",
    "    y = random.random(the_number) + y_offset\n",
    "    label = [label for i in range(the_number)]\n",
    "    return(DataFrame({'x' : x, 'y' : y, 'label' : label}))\n",
    "\n",
    "data = concat([getRanDF(1,1,'r'), getRanDF(1,3,'g'), getRanDF(3,1,'b'), getRanDF(3,3,'purple')])\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.iloc[50:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick scatter shows that yes, these are easy identifiable clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(kind='scatter', x='x', y='y', c=data.label);\n",
    "plt.title('Really Easy Clusters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cls = cluster.k_means(data[ ['x', 'y'] ].values, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cls[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing cls returns two arrays and a float.\n",
    "The first array is 'x' and 'y' for the centrioids of each cluster.\n",
    "The second array is the cluster values.\n",
    "The float represents the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['clusters'] = cls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add colur\n",
    "data['clusters'] = data.clusters.astype('str').map({'0': 'r', '1': 'b', '2': 'g', '3': 'purple'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(kind='scatter', x='x', y='y', c=data.clusters);\n",
    "plt.title('Really Easy Clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we introduce a new length of cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classfive = DataFrame({\n",
    "    'x' : random.random(50) * 50 + 100,\n",
    "    'y' : random.random(50) * 50 + 100,\n",
    "    'label' : ['orange' for i in range(50)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(classfive[['x','y']] - classfive[['x','y']].mean() ) / classfive[['x','y']].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_denorm = concat([data, classfive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = cluster.k_means(data_denorm[ ['x', 'y'] ].values, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_denorm['clusters'] = cls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(data_denorm.x.values, data_denorm.y.values, c=data_denorm.clusters)\n",
    "plt.title('Clusters Identifed by color');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Iris Data Application\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "cls = cluster.k_means([x[2:] for x in iris.data], 3)\n",
    "print cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x[2:] for x in iris.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results here. How did K-means do without any data changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.scatter(iris.data[:,:1], iris.data[:, 2:3], cmap=plt.cm.jet, c=iris.target)\n",
    "plt.subplot(212)\n",
    "plt.scatter(iris.data[:,:1], iris.data[:, 2:3], cmap=plt.cm.jet, c=list(cls[1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the Silhouette Coefficient for the total cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(iris.data, cls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze a dataset containing 13 chemical measurements on 178 Italian wine samples. Get the data from the [UCI Machine Learning Repository](https://gist.githubusercontent.com/tijptjik/9408623/raw/b237fa5848349a14a14e5d4107dc7897c21951f5/wine.csv).\n",
    "\n",
    "* What is the optimal number of clusters?\n",
    "* What determines what clusters consist of?\n",
    "* How well did the K-means clustering uncover the actual structure of the data contained in the first column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://gist.githubusercontent.com/tijptjik/9408623/raw/b237fa5848349a14a14e5d4107dc7897c21951f5/wine.csv\"\n",
    "df_wine = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wine.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/resources.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resource](assets/intro_to_data_mining.png)[Introduction to Data Mining (Ch. 8)](http://www-users.cs.umn.edu/~kumar/dmbook/index.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Academic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [An implementation of the relational k-means algorithm](http://arxiv.org/abs/1304.6899)\n",
    "* [An efficient k-means algorithm integrated with Jaccard distance measure for document clustering](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5340335&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5340335)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [SciKit Clusting](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "* [sklearn.cluster.KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Blogpost on Insult Detection](http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/)\n",
    "* [Github Code of Insult Detection Solution](https://github.com/amueller/kaggle_insults/)\n",
    "* [Choosing a ML Classifier](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/)\n",
    "* [KMeans IPython Notebook](http://nbviewer.ipython.org/urls/raw.github.com/temporaer/tutorial_ml_gkbionics/master/2%2520-%2520KMeans.ipynb)\n",
    "* [Cloudera ML KMeans](http://blog.cloudera.com/blog/2013/03/cloudera_ml_data_science_tools/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
